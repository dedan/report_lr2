%!TEX root = /Users/dedan/bccn/lab_rotations/johannes/report/report.tex
\chapter{Introduction} % (fold)
\label{sg:cha:introduction}

What is attention and what is it good for? It is thought that the brain does not have enough computational resources to compute all sensory stimuli to the same extent and that attention serves as a mechanism to overcome this problem. From early psychology on this phenomenon was investigated \todo{cite james here}, different theories tried to explain what attention is and in the last decades different models where created to explain attention even on a quantitative level.

\section{History of attention research} % (fold)
\label{sg:sec:attention_as_selection}

One of the first and most famous theories on attention is Broadbent's filter theory \todo{cite broadbent here} and it describes attention as a two stage selection mechanism. In the first stage sensory signals are processed with respect to low-level features in a parallel fashion. After certain properties of a low-level feature \emph{attracted} the attention the related sensory signal was continuously processed on a second serial processing stage. The theory describes this second stage to carry out the computationally more expensive tasks like for example object recognition or semantic analysis of a scene. Although this theory explained some phenomena it failed in others as for example the well-known \emph{Cocktail Party effect}. This effect describes the situation when someone is in the middle of a conversation on a cocktail party but suddenly shifts his attention to another one in which someone called his name or mentioned an interesting topic. According to Broadbent's theory this should not be possible as semantic analysis takes place on the second stage after filtering according to low-level features. An improvement is Treisman's Attenuation Model \todo{cite treisman here} which still has two stages and does the first stage processing according to low-level features, but then attenuates the not selected processes rather then eliminating them. It can be also seen as a flashlight on the attended stimulus.
\todo{explain saliency as a model for this low-level selection}
\todo{and cite iti and koch}

That early selection or attenuation is governed by low-level features before expensive high-level analysis is done sounds so reasonable that it was not questioned in the beginning of attention research. Although later experiments surprisingly pointed out that also high-level information can guide attention.
\todo{describe here the two experiments that also johannes mentioned in his talk - Potter 1975 and Biedermann et. al. 1982}

Even more recent research \cite{Einhauser:2008cv} showed that attention might even be object based and this rises the question: \emph{How can we attend to objects before they are recognized as objects?}. And answer two this question is the topic of current research and there are already several proposals of how to solve it. And extension of the \emph{Saliency Model} \todo{cite walter und koch} allows the detection of so-called proto-objects by recurrent connections from the saliency map \todo{validate this from paper walter und koch} and other researchers propose \todo{who?} that mid-level features like boundaries and continuation might help to detect the prototypical objects.

% section attention_as_selection (end) 

\paragraph{Question} % (fold)
\label{par:question}
This last section gave an short overview of attention and possible realizations.It was previously mentioned that attention might serve to overcome the problem of limited computational resources in the brain. But this limit is also a problem we are facing in computer vision and therefore it might be useful to apply attentional mechanisms in computer vision. So the questions this work is dealing with are:

\begin{itemize}
    \item can a simple heuristic be found in order to detect proto-objects?
    \item can this proto-objects help to save computational resources in a computer vision system?
\end{itemize}

% paragraph question (end)
% section introduction (end)
